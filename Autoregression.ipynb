{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGgz3394KerL0pXo2KozvO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/NLP_learning/blob/main/Autoregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hu3sfOdTBjVY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import models, layers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Where the text files are going to live.\n",
        "dataset_path = \"dataset\"\n",
        "dataset_path_all = os.path.join(dataset_path, \"all\")\n",
        "dataset_path_train = os.path.join(dataset_path, \"train\")\n",
        "dataset_path_valid = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "# Just use 20 files.\n",
        "file_number = 20\n",
        "\n",
        "# Gather the corpus if it has not been gathered yet.\n",
        "if not os.path.exists(dataset_path):\n",
        "\n",
        "    # Create all the folders.\n",
        "    for path in [dataset_path, dataset_path_all, dataset_path_train, dataset_path_valid]:\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "    # Clone the repo.\n",
        "    !git clone https://github.com/vilmibm/lovecraftcorpus\n",
        "\n",
        "    # Find all the files.\n",
        "    paths_all = glob.glob(\"lovecraftcorpus/*.txt\")\n",
        "    print(sorted(paths_all))\n",
        "\n",
        "    # Standardize.\n",
        "    for path in paths_all:\n",
        "        content = open(path).read()\n",
        "        content = content.lower()\n",
        "        for punctuation in \".,:;?!\":\n",
        "            content = content.replace(punctuation, \" \" + punctuation)\n",
        "        open(path, \"w\").write(content)\n",
        "\n",
        "    # Do not use all.\n",
        "    paths_all = paths_all[:file_number]\n",
        "\n",
        "    # Split 80/20.\n",
        "    split_index = int(len(paths_all) * 0.8)\n",
        "    paths_train = paths_all[:split_index]\n",
        "    paths_valid = paths_all[split_index:]\n",
        "\n",
        "    # Copy files.\n",
        "    def copy(paths, destination):\n",
        "        for path in paths:\n",
        "            shutil.copy2(path, destination)\n",
        "    copy(paths_all, dataset_path_all)\n",
        "    copy(paths_train, dataset_path_train)\n",
        "    copy(paths_valid, dataset_path_valid)\n",
        "\n",
        "    # Delete repo.\n",
        "    !rm -rf lovecraftcorpus\n",
        "\n",
        "    # Done.\n",
        "    print(\"Corpus downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaN53LpdBuP1",
        "outputId": "9432fece-f5b3-4808-eee1-49421cb3183f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lovecraftcorpus'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 74 (delta 0), reused 3 (delta 0), pack-reused 70 (from 1)\u001b[K\n",
            "Receiving objects: 100% (74/74), 1.12 MiB | 10.52 MiB/s, done.\n",
            "['lovecraftcorpus/alchemist.txt', 'lovecraftcorpus/arthur_jermyn.txt', 'lovecraftcorpus/azathoth.txt', 'lovecraftcorpus/beast.txt', 'lovecraftcorpus/beyond_wall_of_sleep.txt', 'lovecraftcorpus/book.txt', 'lovecraftcorpus/celephais.txt', 'lovecraftcorpus/charles_dexter_ward.txt', 'lovecraftcorpus/clergyman.txt', 'lovecraftcorpus/colour_out_of_space.txt', 'lovecraftcorpus/cool_air.txt', 'lovecraftcorpus/crawling_chaos.txt', 'lovecraftcorpus/cthulhu.txt', 'lovecraftcorpus/dagon.txt', 'lovecraftcorpus/descendent.txt', 'lovecraftcorpus/doorstep.txt', 'lovecraftcorpus/dreams_in_the_witch.txt', 'lovecraftcorpus/dunwich.txt', 'lovecraftcorpus/erich_zann.txt', 'lovecraftcorpus/ex_oblivione.txt', 'lovecraftcorpus/festival.txt', 'lovecraftcorpus/from_beyond.txt', 'lovecraftcorpus/gates_of_silver_key.txt', 'lovecraftcorpus/haunter.txt', 'lovecraftcorpus/he.txt', 'lovecraftcorpus/high_house_mist.txt', 'lovecraftcorpus/hound.txt', 'lovecraftcorpus/hypnos.txt', 'lovecraftcorpus/innsmouth.txt', 'lovecraftcorpus/iranon.txt', 'lovecraftcorpus/juan_romero.txt', 'lovecraftcorpus/kadath.txt', 'lovecraftcorpus/lurking_fear.txt', 'lovecraftcorpus/martins_beach.txt', 'lovecraftcorpus/medusas_coil.txt', 'lovecraftcorpus/memory.txt', 'lovecraftcorpus/moon_bog.txt', 'lovecraftcorpus/mountains_of_madness.txt', 'lovecraftcorpus/nameless.txt', 'lovecraftcorpus/nyarlathotep.txt', 'lovecraftcorpus/old_folk.txt', 'lovecraftcorpus/other_gods.txt', 'lovecraftcorpus/outsider.txt', 'lovecraftcorpus/pharoahs.txt', 'lovecraftcorpus/pickman.txt', 'lovecraftcorpus/picture_house.txt', 'lovecraftcorpus/poetry_of_gods.txt', 'lovecraftcorpus/polaris.txt', 'lovecraftcorpus/randolph_carter.txt', 'lovecraftcorpus/rats_walls.txt', 'lovecraftcorpus/reanimator.txt', 'lovecraftcorpus/redhook.txt', 'lovecraftcorpus/sarnath.txt', 'lovecraftcorpus/shadow_out_of_time.txt', 'lovecraftcorpus/shunned_house.txt', 'lovecraftcorpus/silver_key.txt', 'lovecraftcorpus/street.txt', 'lovecraftcorpus/temple.txt', 'lovecraftcorpus/terrible_old_man.txt', 'lovecraftcorpus/tomb.txt', 'lovecraftcorpus/tree.txt', 'lovecraftcorpus/ulthar.txt', 'lovecraftcorpus/unnamable.txt', 'lovecraftcorpus/vault.txt', 'lovecraftcorpus/what_moon_brings.txt', 'lovecraftcorpus/whisperer.txt', 'lovecraftcorpus/white_ship.txt']\n",
            "Corpus downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLNw0liU8iuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czmqXthL8aTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size  = 32  # not for SGD! This is a different batch size.\n",
        "seed = 667\n",
        "\n",
        "def create_dataset(dataset_path):\n",
        "    dataset = preprocessing.text_dataset_from_directory(\n",
        "        dataset_path,\n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        seed=seed\n",
        "    )\n",
        "    return dataset\n",
        "dataset_original_all = create_dataset(dataset_path_all)\n",
        "dataset_original_train = create_dataset(dataset_path_train)\n",
        "dataset_original_valid = create_dataset(dataset_path_valid)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLn9Hi-UCGcs",
        "outputId": "82158cf8-a64e-429b-9e27-a4912e97fe05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 files.\n",
            "Found 16 files.\n",
            "Found 4 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10_000\n",
        "encoder = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "encoder.adapt(dataset_original_all)\n",
        "vocabulary = encoder.get_vocabulary()\n",
        "print(vocabulary[:100])"
      ],
      "metadata": {
        "id": "n9YifWZd8wu0",
        "outputId": "f99c7b10-79ed-4e02-f5f2-084fa200e09e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', ',', 'of', '.', 'and', 'to', 'a', 'in', 'was', 'that', 'had', 'he', 'i', 'it', 'as', 'his', 'with', 'at', 'which', 'on', 'from', 'for', 'we', 'not', 'were', ';', 'but', 'by', 'all', 'be', 'this', 'they', 'my', 'have', 'or', 'could', 'one', 'there', 'him', 'been', 'when', 'an', 'our', 'some', 'no', 'their', 'would', 'old', 'what', 'me', 'about', 'so', 'more', 'is', 'its', 'now', 'seemed', 'out', 'up', 'only', 'did', 'into', 'than', 'those', 'through', 'though', 'them', 'even', 'other', 'after', 'time', 'very', 'who', 'great', 'before', 'any', 'must', 'like', 'things', 'then', 'over', 'if', 'these', 'us', 'came', 'where', 'saw', 'found', 'man', 'whose', 'down', 'certain', 'such', 'yet', 'made', 'might', 'beyond', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequence_length = 32\n",
        "vocabulary = encoder.get_vocabulary()\n",
        "padding_token_id = 0\n",
        "\n",
        "\n",
        "def create_dataset_for_autoregression(dataset):\n",
        "  x_inputs = []\n",
        "  y_outputs = []\n",
        "  for stories in dataset:\n",
        "    stories = encoder(stories).numpy() # Does the padding\n",
        "\n",
        "    for story in stories:\n",
        "      story = [index for index in list(story) if index != padding_token_id] # removes padding\n",
        "\n",
        "      # Allowing to generate from sequences that are shorter than sequence length.\n",
        "      padding = [padding_token_id] * sequence_length\n",
        "      story = padding + story\n",
        "\n",
        "      for start_index in range(0,len(story)-sequence_length): # no overflow.\n",
        "          x = story[start_index:start_index + sequence_length]\n",
        "          y = story[start_index + 1 :start_index + sequence_length+1]\n",
        "          assert len(y) == sequence_length , \"should not happen\"\n",
        "      assert False\n",
        "  # Done,\n",
        "  return tf.data.Dataset.from_tensor_slices([x_inputs, y_outputs])\n",
        "\n",
        "\n",
        "dataset_train = create_dataset_for_autoregression(dataset_original_train)\n",
        "#dataset_valid = create_dataset_for_autoregression(dataset_original_valid)"
      ],
      "metadata": {
        "id": "FpPokyGL8t_r",
        "outputId": "a71db27c-9937-4ee0-e737-0dcdbfea5465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1 63  2 ...  0  0  0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-935e3b112d28>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset_for_autoregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_original_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#dataset_valid = create_dataset_for_autoregression(dataset_original_valid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-935e3b112d28>\u001b[0m in \u001b[0;36mcreate_dataset_for_autoregression\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstories\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mstory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpadding_token_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# removes padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8QaNeuL70-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}