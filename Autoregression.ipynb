{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu69Q9cIvcv4DEH8E4mXPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/NLP_learning/blob/main/Autoregression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hu3sfOdTBjVY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import models, layers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Where the text files are going to live.\n",
        "dataset_path = \"dataset\"\n",
        "dataset_path_all = os.path.join(dataset_path, \"all\")\n",
        "dataset_path_train = os.path.join(dataset_path, \"train\")\n",
        "dataset_path_valid = os.path.join(dataset_path, \"valid\")\n",
        "\n",
        "# Just use 20 files.\n",
        "file_number = 20\n",
        "\n",
        "# Gather the corpus if it has not been gathered yet.\n",
        "if not os.path.exists(dataset_path):\n",
        "\n",
        "    # Create all the folders.\n",
        "    for path in [dataset_path, dataset_path_all, dataset_path_train, dataset_path_valid]:\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "    # Clone the repo.\n",
        "    !git clone https://github.com/vilmibm/lovecraftcorpus\n",
        "\n",
        "    # Find all the files.\n",
        "    paths_all = glob.glob(\"lovecraftcorpus/*.txt\")\n",
        "    print(sorted(paths_all))\n",
        "\n",
        "    # Standardize.\n",
        "    for path in paths_all:\n",
        "        content = open(path).read()\n",
        "        content = content.lower()\n",
        "        for punctuation in \".,:;?!\":\n",
        "            content = content.replace(punctuation, \" \" + punctuation)\n",
        "        open(path, \"w\").write(content)\n",
        "\n",
        "    # Do not use all.\n",
        "    paths_all = paths_all[:file_number]\n",
        "\n",
        "    # Split 80/20.\n",
        "    split_index = int(len(paths_all) * 0.8)\n",
        "    paths_train = paths_all[:split_index]\n",
        "    paths_valid = paths_all[split_index:]\n",
        "\n",
        "    # Copy files.\n",
        "    def copy(paths, destination):\n",
        "        for path in paths:\n",
        "            shutil.copy2(path, destination)\n",
        "    copy(paths_all, dataset_path_all)\n",
        "    copy(paths_train, dataset_path_train)\n",
        "    copy(paths_valid, dataset_path_valid)\n",
        "\n",
        "    # Delete repo.\n",
        "    !rm -rf lovecraftcorpus\n",
        "\n",
        "    # Done.\n",
        "    print(\"Corpus downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaN53LpdBuP1",
        "outputId": "f6e2b0e8-ffb7-45ac-cc0e-de6376b3afc3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lovecraftcorpus'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 74 (delta 0), reused 3 (delta 0), pack-reused 70 (from 1)\u001b[K\n",
            "Receiving objects: 100% (74/74), 1.12 MiB | 2.89 MiB/s, done.\n",
            "['lovecraftcorpus/alchemist.txt', 'lovecraftcorpus/arthur_jermyn.txt', 'lovecraftcorpus/azathoth.txt', 'lovecraftcorpus/beast.txt', 'lovecraftcorpus/beyond_wall_of_sleep.txt', 'lovecraftcorpus/book.txt', 'lovecraftcorpus/celephais.txt', 'lovecraftcorpus/charles_dexter_ward.txt', 'lovecraftcorpus/clergyman.txt', 'lovecraftcorpus/colour_out_of_space.txt', 'lovecraftcorpus/cool_air.txt', 'lovecraftcorpus/crawling_chaos.txt', 'lovecraftcorpus/cthulhu.txt', 'lovecraftcorpus/dagon.txt', 'lovecraftcorpus/descendent.txt', 'lovecraftcorpus/doorstep.txt', 'lovecraftcorpus/dreams_in_the_witch.txt', 'lovecraftcorpus/dunwich.txt', 'lovecraftcorpus/erich_zann.txt', 'lovecraftcorpus/ex_oblivione.txt', 'lovecraftcorpus/festival.txt', 'lovecraftcorpus/from_beyond.txt', 'lovecraftcorpus/gates_of_silver_key.txt', 'lovecraftcorpus/haunter.txt', 'lovecraftcorpus/he.txt', 'lovecraftcorpus/high_house_mist.txt', 'lovecraftcorpus/hound.txt', 'lovecraftcorpus/hypnos.txt', 'lovecraftcorpus/innsmouth.txt', 'lovecraftcorpus/iranon.txt', 'lovecraftcorpus/juan_romero.txt', 'lovecraftcorpus/kadath.txt', 'lovecraftcorpus/lurking_fear.txt', 'lovecraftcorpus/martins_beach.txt', 'lovecraftcorpus/medusas_coil.txt', 'lovecraftcorpus/memory.txt', 'lovecraftcorpus/moon_bog.txt', 'lovecraftcorpus/mountains_of_madness.txt', 'lovecraftcorpus/nameless.txt', 'lovecraftcorpus/nyarlathotep.txt', 'lovecraftcorpus/old_folk.txt', 'lovecraftcorpus/other_gods.txt', 'lovecraftcorpus/outsider.txt', 'lovecraftcorpus/pharoahs.txt', 'lovecraftcorpus/pickman.txt', 'lovecraftcorpus/picture_house.txt', 'lovecraftcorpus/poetry_of_gods.txt', 'lovecraftcorpus/polaris.txt', 'lovecraftcorpus/randolph_carter.txt', 'lovecraftcorpus/rats_walls.txt', 'lovecraftcorpus/reanimator.txt', 'lovecraftcorpus/redhook.txt', 'lovecraftcorpus/sarnath.txt', 'lovecraftcorpus/shadow_out_of_time.txt', 'lovecraftcorpus/shunned_house.txt', 'lovecraftcorpus/silver_key.txt', 'lovecraftcorpus/street.txt', 'lovecraftcorpus/temple.txt', 'lovecraftcorpus/terrible_old_man.txt', 'lovecraftcorpus/tomb.txt', 'lovecraftcorpus/tree.txt', 'lovecraftcorpus/ulthar.txt', 'lovecraftcorpus/unnamable.txt', 'lovecraftcorpus/vault.txt', 'lovecraftcorpus/what_moon_brings.txt', 'lovecraftcorpus/whisperer.txt', 'lovecraftcorpus/white_ship.txt']\n",
            "Corpus downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLNw0liU8iuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czmqXthL8aTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size  = 32  # not for SGD! This is a different batch size.\n",
        "seed = 667\n",
        "\n",
        "def create_dataset(dataset_path):\n",
        "    dataset = preprocessing.text_dataset_from_directory(\n",
        "        dataset_path,\n",
        "        labels=None,\n",
        "        batch_size=batch_size,\n",
        "        seed=seed\n",
        "    )\n",
        "    return dataset\n",
        "dataset_original_all = create_dataset(dataset_path_all)\n",
        "dataset_original_train = create_dataset(dataset_path_train)\n",
        "dataset_original_valid = create_dataset(dataset_path_valid)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLn9Hi-UCGcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9cdcc3-0143-43f1-ccfc-8cf2a23c2e7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 files.\n",
            "Found 16 files.\n",
            "Found 4 files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10_000\n",
        "encoder = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=None,\n",
        "    split=\"whitespace\",\n",
        "    output_mode=\"int\"\n",
        ")\n",
        "encoder.adapt(dataset_original_all)\n",
        "vocabulary = encoder.get_vocabulary()\n",
        "print(vocabulary[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9YifWZd8wu0",
        "outputId": "b0db291d-36c3-424f-b866-eb57c97017d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', ',', 'of', '.', 'and', 'to', 'a', 'in', 'was', 'that', 'had', 'he', 'i', 'it', 'as', 'his', 'with', 'at', 'which', 'on', 'from', 'for', 'we', 'not', 'were', ';', 'but', 'by', 'all', 'be', 'this', 'they', 'my', 'have', 'or', 'could', 'one', 'there', 'him', 'been', 'when', 'an', 'our', 'some', 'no', 'their', 'would', 'old', 'what', 'me', 'about', 'so', 'more', 'is', 'its', 'now', 'seemed', 'out', 'up', 'only', 'did', 'into', 'than', 'those', 'through', 'though', 'them', 'even', 'other', 'after', 'time', 'very', 'who', 'great', 'before', 'any', 'must', 'like', 'things', 'then', 'over', 'if', 'these', 'us', 'came', 'where', 'saw', 'found', 'man', 'whose', 'down', 'certain', 'such', 'yet', 'made', 'might', 'beyond', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequence_length = 32\n",
        "vocabulary = encoder.get_vocabulary()\n",
        "padding_token_id = 0\n",
        "\n",
        "\n",
        "def create_dataset_for_autoregression(dataset):\n",
        "  x_inputs = []\n",
        "  y_outputs = []\n",
        "  for stories in dataset:\n",
        "    stories = encoder(stories).numpy() # Does the padding\n",
        "\n",
        "    for story in stories:\n",
        "      story = [index for index in list(story) if index != padding_token_id] # removes padding\n",
        "\n",
        "      # Allowing to generate from sequences that are shorter than sequence length.\n",
        "      padding = [padding_token_id] * sequence_length\n",
        "      story = padding + story\n",
        "\n",
        "      for start_index in range(0,len(story)-sequence_length): # no overflow.\n",
        "          x = story[start_index:start_index + sequence_length]\n",
        "          assert len(x) == sequence_length, \"Should not happen.\"\n",
        "          y = story[start_index + 1 :start_index + sequence_length+1]\n",
        "          assert len(y) == sequence_length , \"should not happen\"\n",
        "\n",
        "          x_inputs += [x]\n",
        "          y_outputs += [y]\n",
        "\n",
        "\n",
        "  # Done,\n",
        "  return tf.data.Dataset.from_tensor_slices((x_inputs, y_outputs))\n",
        "\n",
        "\n",
        "dataset_train = create_dataset_for_autoregression(dataset_original_train)\n",
        "dataset_valid = create_dataset_for_autoregression(dataset_original_valid)"
      ],
      "metadata": {
        "id": "FpPokyGL8t_r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(indices):\n",
        "  return \" \".join([vocabulary[index] for index in indices if vocabulary[index] != \"\"])\n",
        "\n",
        "for input,output in dataset_train.take(20):\n",
        "  print(decode(input))\n",
        "  print(decode(output))\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8QaNeuL70-j",
        "outputId": "411e784e-6d07-4411-e168-7080b0eaceb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "the\n",
            "\n",
            "the\n",
            "the terrible\n",
            "\n",
            "the terrible\n",
            "the terrible old\n",
            "\n",
            "the terrible old\n",
            "the terrible old man\n",
            "\n",
            "the terrible old man\n",
            "the terrible old man the\n",
            "\n",
            "the terrible old man the\n",
            "the terrible old man the inhabitants\n",
            "\n",
            "the terrible old man the inhabitants\n",
            "the terrible old man the inhabitants of\n",
            "\n",
            "the terrible old man the inhabitants of\n",
            "the terrible old man the inhabitants of kingsport\n",
            "\n",
            "the terrible old man the inhabitants of kingsport\n",
            "the terrible old man the inhabitants of kingsport say\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say\n",
            "the terrible old man the inhabitants of kingsport say and\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and\n",
            "the terrible old man the inhabitants of kingsport say and think\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think\n",
            "the terrible old man the inhabitants of kingsport say and think many\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many\n",
            "the terrible old man the inhabitants of kingsport say and think many things\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things\n",
            "the terrible old man the inhabitants of kingsport say and think many things about\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old man\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old man\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old man which\n",
            "\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old man which\n",
            "the terrible old man the inhabitants of kingsport say and think many things about the terrible old man which generally\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def render_history(history):\n",
        "    plt.title(\"Training loss vs. validation loss\")\n",
        "    plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.title(\"Training accuracy vs. validation accuracy\")\n",
        "    plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "aF86Ta92YmGy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Ej8L9ZnhY7Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 128\n"
      ],
      "metadata": {
        "id": "-S3saQ4_AZKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}