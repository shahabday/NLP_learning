{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/NLP_learning/blob/main/DSR_41_Introduction_NLP_deeplearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-fv-56Bk_ls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8tobE6IljY7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvEpQ-xklkZd"
      },
      "source": [
        "# Notes go here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E816THZAll-m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models , layers\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MSUADsNupMBM"
      },
      "outputs": [],
      "source": [
        "#!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha0UkaBYpXVb"
      },
      "outputs": [],
      "source": [
        "datasets,info = tfds.load(\n",
        "\n",
        "          \"imdb_reviews/plain_text\",\n",
        "          split=['train','test[:50%]','test[50%:]'], # only in tensorflow, would\n",
        "                                                     #be nice to have it in numpy\n",
        "          as_supervised = True,\n",
        "          with_info = True\n",
        ") # we have three datasets , training , test, validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghJ2jLzVqPqc"
      },
      "outputs": [],
      "source": [
        "datasets_train_original = datasets [0]\n",
        "dataset_validate_original = datasets [1]\n",
        "dataset_test_original = datasets [2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsX6txArLlK"
      },
      "outputs": [],
      "source": [
        "for x,y in datasets_train_original.take(4):\n",
        "  print( x.numpy().decode('utf-8') )\n",
        "  print (y.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk4FWA8rwnI1"
      },
      "outputs": [],
      "source": [
        "lengths = []\n",
        "for x, _ in datasets_train_original:\n",
        "  length = len(x.numpy().decode('utf-8').split())\n",
        "  lengths.append(length)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(lengths, bins = 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cl0OVbazISH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shortest_sample = ' ' * 100_000\n",
        "longest_sample = ''\n",
        "\n",
        "for x,y in datasets_train_original :\n",
        "  x = x.numpy().decode('utf-8')\n",
        "  if len(x) < len(shortest_sample) :\n",
        "    shortest_sample = x\n",
        "  if len(x) > len(longest_sample) :\n",
        "    longest_sample = x\n",
        "\n",
        "print (shortest_sample)\n",
        "print (longest_sample)\n"
      ],
      "metadata": {
        "id": "UT_vjHgSI53_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "\n",
        "for _,y in datasets_train_original :\n",
        "  labels.append(y.numpy())\n",
        "\n",
        "plt.hist(labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jPJBTxSKPEj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 1000\n",
        "\n",
        "encoder = layers.TextVectorization(\n",
        "    max_tokens = vocabulary_size,\n",
        "    standardize = 'lower_and_strip_punctuation',\n",
        "    split = \"whitespace\",\n",
        "    output_mode= 'multi_hot'\n",
        ")\n",
        "\n",
        "encoder.adapt(datasets_train_original.map(lambda text, label : text).batch(2048))"
      ],
      "metadata": {
        "id": "Ax_uLBGNOG98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.get_vocabulary().index('world')"
      ],
      "metadata": {
        "id": "9ci0mK1pauHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder.get_vocabulary())"
      ],
      "metadata": {
        "id": "vvNTyee6VDVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(encoder('I am very happy to be learning at DSR. MOdSHT').numpy()))"
      ],
      "metadata": {
        "id": "tFkZDTzdVGhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0g0BsQtkWSOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = models.Sequential()\n",
        "# model.add(encoder)\n",
        "# model.build(input_shape=(None,))\n",
        "# model.summary()\n",
        "\n",
        "# model.predict(tf.constant(['hello world because I want to']))"
      ],
      "metadata": {
        "id": "YfiTrPgbcyle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = datasets_train_original.cache() # lift it to memory\n",
        "dataset_train = dataset_train.shuffle(25000)\n",
        "dataset_train = dataset_train.batch(32)\n",
        "\n",
        "dataset_validate = dataset_test_original.cache()\n",
        "dataset_validate = dataset_validate.batch(32)\n",
        "\n",
        "dataset_test = dataset_test_original.cache()\n",
        "dataset_test = dataset_test.batch(32)\n",
        "\n"
      ],
      "metadata": {
        "id": "ganWGFLGtE7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(encoder)\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
        "#model.add(layers.Dense(2,activation = 'softmax'))\n",
        "model.build(input_shape=(None,))\n",
        "model.summary()\n",
        "\n",
        "model.predict(tf.constant(['hello world because I want to']))\n",
        "model.compile(\n",
        "              optimizer = 'adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "print(model.evaluate(dataset_test))\n",
        "\n",
        "history = model.fit(\n",
        "\n",
        "          dataset_train ,\n",
        "          epochs= 10 ,\n",
        "          validation_data=dataset_validate\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(model.evaluate(dataset_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "Zyxih-bRc8B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_history(history):\n",
        "  plt.plot(history[\"loss\"],label='loss')\n",
        "  plt.plot(history['val_loss'], label = 'val_loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  plt.plot(history[\"accuracy\"],label='accuracy')\n",
        "  plt.plot(history['val_accuracy'], label = 'val_accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "mcjEbOBLf0bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding"
      ],
      "metadata": {
        "id": "ePwlxjNy0JZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = 10_000\n",
        "sequence_length = 128 # AKA context size\n",
        "\n",
        "encoder = layers.TextVectorization(\n",
        "                                  max_tokens = vocabulary_size,\n",
        "                                  output_sequence_length  = sequence_length, # new !\n",
        "                                   standardize = 'lower_and_strip_punctuation',\n",
        "                                   split = 'whitespace',\n",
        "                                  output_mode = 'int' # new\n",
        "\n",
        ")\n",
        "\n",
        "encoder.adapt(datasets_train_original.map(lambda text, label : text).batch(2048))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jv_tz61mzzBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder.get_vocabulary()[:20])"
      ],
      "metadata": {
        "id": "hasVHb7ysYsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder('Is this the real life isthis just fantasy caught in a landslide . no escape from reality. seeee , MAMA rapsody integrity complexity rare reality matrix mask offside corner steven gerrard table')"
      ],
      "metadata": {
        "id": "4p_zAWNu3GRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(encoder)\n",
        "model.add(layers.Embedding(input_dim=vocabulary_size, output_dim=32))\n",
        "model.add(layers.Reshape((2048,)))\n",
        "model.add(layers.Dense(32, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "model.build(input_shape=(None,))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "#print(model.evaluate(dataset_test))\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=10,\n",
        "    validation_data=dataset_validate\n",
        ")\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "oz1lgeak3ZxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.layers[1].get_weights()[0].shape"
      ],
      "metadata": {
        "id": "cFz06MUqlPQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.imshow(model.layers[1].get_weights()[0][:100,::],cmap=\"inferno\")"
      ],
      "metadata": {
        "id": "g6MNEQ1zlQEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mport numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# List of target words\n",
        "words = ['germany', 'france', 'england', 'berlin', 'paris', 'london']\n",
        "words = [\n",
        "    'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world', 'life', 'hand',\n",
        "    'part', 'child', 'eye', 'woman', 'place', 'work', 'week', 'case', 'point', 'government',\n",
        "    'company', 'number', 'group', 'problem', 'fact', 'be', 'have', 'do', 'say', 'get'\n",
        "]\n",
        "\n",
        "\n",
        "# Retrieve the vocabulary from the embedding layer\n",
        "vocab = model.layers[0].get_vocabulary()\n",
        "\n",
        "# Get indices for the target words\n",
        "indices = [vocab.index(word) for word in words if word in vocab]\n",
        "\n",
        "# Extract embeddings for the target words\n",
        "embeddings = model.layers[1].get_weights()[0]  # Assuming the embedding layer is the second layer\n",
        "selected_embeddings = np.array([embeddings[idx] for idx in indices])\n",
        "\n",
        "# Initialize t-SNE with desired parameters\n",
        "tsne = TSNE(n_components=2, perplexity=1, n_iter=1000, random_state=42)\n",
        "\n",
        "# Fit and transform the embeddings\n",
        "X_embedded = tsne.fit_transform(selected_embeddings)\n"
      ],
      "metadata": {
        "id": "T7QkK0PLlSF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], color='blue')\n",
        "\n",
        "# Annotate each point with the corresponding word\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (X_embedded[i, 0], X_embedded[i, 1]), fontsize=12)\n",
        "\n",
        "# Set plot title and labels\n",
        "plt.title('t-SNE Visualization of Word Embeddings')\n",
        "plt.xlabel('t-SNE Dimension 1')\n",
        "plt.ylabel('t-SNE Dimension 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TeAbAwkNlYCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm70dNA66qRE09s60CERGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}